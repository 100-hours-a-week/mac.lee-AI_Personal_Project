# mac.lee-AI_Personal_Project

---
# Using Potato Plant Diseases Data to Build CNN Modeling Comparison
[Date : 2025.03]

---

## 1. ì„œë¡ 

---

- ê°ìëŠ” ì„¸ê³„ ì£¼ìš” ì‹ëŸ‰ ì‘ë¬¼ ì¤‘ í•˜ë‚˜ë¡œ, íŠ¹íˆ **ì—­ë³‘(Late Blight, Phytophthora infestans)** ì€ ê°ì ìƒì‚°ëŸ‰ì— ì¹˜ëª…ì ì¸ ì˜í–¥ì„ ë¯¸ì¹˜ëŠ” ëŒ€í‘œì ì¸ ë³‘í•´ì…ë‹ˆë‹¤. íŠ¹íˆ ì´ˆê¸° ì—­ë³‘ê³¼ í›„ê¸° ì—­ë³‘ì€ ì™„ì „íˆ ë‹¤ë¥¸ ì§ˆë³‘ì´ë©° **ì´ˆê¸° ì—­ë³‘**ì€ ìì˜ ì‘ì€ ë°˜ì ì´ë‚˜ ëª¨ì„œë¦¬ ë³€ìƒ‰ ë“±ìœ¼ë¡œ ì‹œì‘ë˜ì–´, ë¹ ë¥¸ ì‹œê°„ ë‚´ì— ì¤„ê¸° ë° ë¿Œë¦¬ê¹Œì§€ ì „íŒŒëœë‹¤. **í›„ê¸° ì—­ë³‘**ì€ ì´ë¯¸ ë³‘ì´ í™•ì‚°ëœ ìƒíƒœì—ì„œ ê¸‰ê²©í•œ ì¡°ì§ ê´´ì‚¬ì™€ ì‘ë¬¼ ê³ ì‚¬ í˜„ìƒì„ ìœ ë°œí•œë‹¤. ì´ ë‘˜ì€ ë°©ì œ ì‹œê¸° ë° ì•½ì œ ì„ íƒì´ ë‹¤ë¥´ê¸° ë•Œë¬¸ì— **ì •í™•í•œ êµ¬ë¶„ì´ í•„ìˆ˜ì ì´ë‹¤**. ë”°ë¼ì„œ ë³¸ ì—°êµ¬ëŠ” **ê°ìì˜ ì´ˆê¸° ë° í›„ê¸° ì—­ë³‘ ë°œìƒ ì‹œê¸°ì˜ ì •í™•í•œ ë¶„ë¥˜ ë° ì˜ˆì¸¡ ëª¨ë¸**ì„ êµ¬ì¶•í•¨ìœ¼ë¡œì¨, ì¡°ê¸° ë°©ì œ ë° ë†ê°€ ìƒì‚°ì„± í–¥ìƒì— ê¸°ì—¬í•˜ê³ ì í•œë‹¤.

## 2. ë°ì´í„°ì…‹ ì„¤ëª…

---

- ì´ë²ˆ ëª¨ë¸ì—ì„œ ì‚¬ìš©í•œ ë°ì´í„° ì…‹ì€ Kaggle ì‚¬ì´íŠ¸ì— ì—…ë¡œë“œ ë˜ì–´ìˆëŠ” â€œ**Potato Plant Diseases Dataâ€** ì´ë‹¤.
- êµ¬ê¸€ ë“œë¼ì´ë¸Œì— ë°ì´í„° ì…‹ì„ ì—…ë¡œë“œ í›„, ë¶ˆëŸ¬ì˜¤ëŠ” ì‹ìœ¼ë¡œ ì§„í–‰í•˜ì˜€ë‹¤.
    
    ```python
    import os
    
    # êµ¬ê¸€ ë“œë¼ì´ë¸Œ ë°ì´í„°ì…‹ ê²½ë¡œ
    dataset_path = "/content/drive/MyDrive/Colab Notebooks/[á„á…¡á„á…¡á„‹á…© á„‡á…®á„á…³á„á…¢á†·á„‘á…³]/[á„á…¡á„á…¡á„‹á…© á„‡á…®á„á…³á„á…¢á†·á„‘á…³] á„€á…¢á„‹á…µá†«á„€á…ªá„Œá…¦2 á„‘á…©á†¯á„ƒá…¥/PlantVillage"
    
    # í´ë˜ìŠ¤ë³„ ì´ë¯¸ì§€ ê°œìˆ˜ ì¶œë ¥ í•¨ìˆ˜
    def count_images_per_class(dataset_path):
        print(f"ê¸°ì¤€ ê²½ë¡œ: {dataset_path}\n")
        for class_name in os.listdir(dataset_path):
            class_path = os.path.join(dataset_path, class_name)
            if os.path.isdir(class_path):
                img_count = len([
                    f for f in os.listdir(class_path)
                    if f.lower().endswith(('.jpg', '.jpeg', '.png'))
                ])
                print(f"í´ë˜ìŠ¤ '{class_name}': {img_count}ì¥")
    
    # ì‹¤í–‰
    count_images_per_class(dataset_path)
    
    # í´ë˜ìŠ¤ë³„ í´ë” í™•ì¸ (íŒŒì¼ì´ ì•„ë‹Œ í´ë”ë§Œ ë¦¬ìŠ¤íŠ¸ì— ì¶”ê°€)
    categories = [category for category in os.listdir(dataset_path) if os.path.isdir(os.path.join(dataset_path, category))]
    
    # í´ë˜ìŠ¤ ëª©ë¡ ì¶œë ¥
    print(f"í´ë˜ìŠ¤ ê°œìˆ˜: {len(categories)}")
    print(f"í´ë˜ìŠ¤ ëª©ë¡: {categories}")
    
    # ì¶œë ¥ ê²°ê³¼
    í´ë˜ìŠ¤ ê°œìˆ˜: 3
    í´ë˜ìŠ¤ ëª©ë¡: ['Potato___Early_blight', 'Potato___Late_blight', 'Potato___healthy']
    
    í´ë˜ìŠ¤ 'Potato___Early_blight': 1000ì¥
    í´ë˜ìŠ¤ 'Potato___Late_blight': 1000ì¥
    í´ë˜ìŠ¤ 'Potato___healthy': 152ì¥
    ```
    
- í•´ë‹¹ ë°ì´í„° ì…‹ì€
    - 'Potato___Early_blight'
    - 'Potato___Late_blight'
    - 'Potato___healthy'
    
    ìœ¼ë¡œ ì´ 3ê°œ í´ë˜ìŠ¤ ì´ë©°, ê°ê° 1000ì¥ ê·¸ë¦¬ê³  152ì¥ìœ¼ë¡œ êµ¬ì„± ë˜ì–´ìˆë‹¤.
    
- ë°ì´í„°ì–‘ì´ ë§¤ìš° ì ì€ í¸ì´ë¯€ë¡œ **ë°ì´í„° ì¦ê°•** ë° **ë°ì´í„° ë¶„í• **ì„ train : val : test = 8:1:1ë¡œ êµ¬ì„±í•˜ì—¬ train í•™ìŠµì— ì¢€ë” ì§‘ì¤‘í•˜ì˜€ë‹¤.
    
    ```python
    import os
    import torch
    from torchvision import transforms, datasets
    from torch.utils.data import DataLoader
    
    # ë°ì´í„°ì…‹ ê²½ë¡œ ì„¤ì •
    DATASET_PATH = "/content/drive/MyDrive/Colab Notebooks/[á„á…¡á„á…¡á„‹á…© á„‡á…®á„á…³á„á…¢á†·á„‘á…³]/[á„á…¡á„á…¡á„‹á…© á„‡á…®á„á…³á„á…¢á†·á„‘á…³] á„€á…¢á„‹á…µá†«á„€á…ªá„Œá…¦2 á„‘á…©á†¯á„ƒá…¥/Split_PlantVillage(8:1:1)"
    
    # í›ˆë ¨ ë°ì´í„°ì— ë°ì´í„° ì¦ê°• ì¶”ê°€
    train_transform = transforms.Compose([
        transforms.RandomResizedCrop(224),  # ì¤‘ì‹¬ í¬ë¡­ í›„ 224x224 ë³€í™˜
        transforms.RandomHorizontalFlip(),  # ì¢Œìš° ë°˜ì „
        transforms.RandomRotation(15),  # 15ë„ ì´ë‚´ ëœë¤ íšŒì „
        transforms.RandomApply([transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1)], p=0.5),  # 50% í™•ë¥ ë¡œ ìƒ‰ê° ë³€í™˜
        transforms.RandomAffine(degrees=15, translate=(0.1, 0.1), scale=(0.8, 1.2)),  # ì´ë™, í™•ëŒ€/ì¶•ì†Œ
        transforms.GaussianBlur(kernel_size=3),  # ë¸”ëŸ¬ íš¨ê³¼ ì¶”ê°€
        transforms.ToTensor(), # Tensorë¡œ ë³€í™˜
        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])
    ])
    
    # ê²€ì¦ & í…ŒìŠ¤íŠ¸ ë°ì´í„°ëŠ” ì›ë³¸ ìœ ì§€
    val_test_transform = transforms.Compose([
        transforms.Resize(256),
        transforms.CenterCrop(224),
        transforms.ToTensor(),
        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])
    ])
    
    # ë°ì´í„°ì…‹ ë¡œë“œ (í›ˆë ¨ ë°ì´í„°ì—ëŠ” ì¦ê°• ì ìš©)
    train_dataset = datasets.ImageFolder(root=os.path.join(DATASET_PATH, "train"), transform=train_transform)
    val_dataset = datasets.ImageFolder(root=os.path.join(DATASET_PATH, "val"), transform=val_test_transform)
    test_dataset = datasets.ImageFolder(root=os.path.join(DATASET_PATH, "test"), transform=val_test_transform)
    
    # ë°ì´í„° ë¡œë” ìƒì„±
    BATCH_SIZE = 32
    train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=4)  # ğŸ”¥ í›ˆë ¨ ë°ì´í„°ëŠ” ì„ìŒ
    val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=4)  # ê²€ì¦ ë°ì´í„°ëŠ” ìœ ì§€
    test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=4)  # í…ŒìŠ¤íŠ¸ ë°ì´í„°ë„ ìœ ì§€
    
    # ë°ì´í„°ì…‹ ê°œìˆ˜ í™•ì¸
    print(f"í›ˆë ¨ ë°ì´í„° ê°œìˆ˜: {len(train_dataset)}")
    print(f"ê²€ì¦ ë°ì´í„° ê°œìˆ˜: {len(val_dataset)}")
    print(f"í…ŒìŠ¤íŠ¸ ë°ì´í„° ê°œìˆ˜: {len(test_dataset)}")
    
    # ì¶œë ¥ ê²°ê³¼
    í›ˆë ¨ ë°ì´í„° ê°œìˆ˜: 2064
    ê²€ì¦ ë°ì´í„° ê°œìˆ˜: 409
    í…ŒìŠ¤íŠ¸ ë°ì´í„° ê°œìˆ˜: 408
    ```
    

## 3. ëª¨ë¸ ì„¤ëª…

---

- ì´ë²ˆ í”„ë¡œì íŠ¸ì—ì„œ ê°ì ì‹ë¬¼ ì—­ë³‘ ë°ì´í„°ì…‹ì„ í•™ìŠµì‹œí‚¤ê¸°ìœ„í•´ í›ˆë ¨í•œ ëª¨ë¸ì€ ì´ 5ê°€ì§€ ëª¨ë¸ë¡œ ResNet50, ResNet18,  VGG16, MobileNet, GoogLeNet ë“±ì„ ì‚¬ìš©í–ˆë‹¤.
    - ResNet50
    - ì¡°ê¸°ì¤‘ë‹¨(early stopping)ì„ ì ìš©í•œ ëª¨ë¸í•™ìŠµ í•¨ìˆ˜
        
        ```python
        import os
        import numpy as np
        import torch
        
        best_model_path = "/content/drive/MyDrive/Colab Notebooks/[á„á…¡á„á…¡á„‹á…© á„‡á…®á„á…³á„á…¢á†·á„‘á…³]/[á„á…¡á„á…¡á„‹á…© á„‡á…®á„á…³á„á…¢á†·á„‘á…³] á„€á…¢á„‹á…µá†«á„€á…ªá„Œá…¦2 á„‘á…©á†¯á„ƒá…¥/ ResNet50_Best_Model(8:1:1).pth"
        
        # ì–¼ë¦¬ ìŠ¤íƒ‘ ì„¤ì •
        patience = 5  # 5 ì—í¬í¬ ë™ì•ˆ ê°œì„ ì´ ì—†ìœ¼ë©´ ì¤‘ë‹¨
        min_delta = 0.001  # ê°œì„ ì´ min_delta ì´í•˜ì´ë©´ ì˜ë¯¸ ì—†ëŠ” ê°œì„ ìœ¼ë¡œ ê°„ì£¼
        best_val_loss = np.inf  # ì²˜ìŒì—ëŠ” ë¬´í•œëŒ€ë¡œ ì„¤ì •
        counter = 0  # ê°œì„ ë˜ì§€ ì•Šì€ íšŸìˆ˜ ì¹´ìš´íŠ¸
        
        EPOCHS = 50  # ìµœëŒ€ 50 ì—í¬í¬ê¹Œì§€ í•™ìŠµ
        
        train_losses = []
        val_losses = []
        train_accuracies = []
        val_accuracies = []
        
        # ëª¨ë¸ í‰ê°€ í•¨ìˆ˜
        def evaluate(model, dataloader):
            model.eval()  # í‰ê°€ ëª¨ë“œë¡œ ë³€ê²½
            correct = 0
            total = 0
            running_loss = 0.0
        
            with torch.no_grad():  # ê·¸ë˜ë””ì–¸íŠ¸ ê³„ì‚° ë¹„í™œì„±í™” (ì†ë„ ìµœì í™”)
                for images, labels in dataloader:
                    images, labels = images.to(device), labels.to(device)
                    outputs = model(images)
                    loss = criterion(outputs, labels)
                    running_loss += loss.item()
        
                    _, predicted = torch.max(outputs, 1)
                    total += labels.size(0)
                    correct += (predicted == labels).sum().item()
        
            avg_loss = running_loss / len(dataloader)
            accuracy = correct / total * 100
            return avg_loss, accuracy
        
        for epoch in range(EPOCHS):
            model.train()
            running_loss = 0.0
            correct = 0
            total = 0
        
            for images, labels in train_loader:
                images, labels = images.to(device), labels.to(device)
        
                optimizer.zero_grad()
                outputs = model(images)
                loss = criterion(outputs, labels)
                loss.backward()
                optimizer.step()
        
                _, predicted = torch.max(outputs, 1)
                total += labels.size(0)
                correct += (predicted == labels).sum().item()
                running_loss += loss.item()
        
            # í•™ìŠµ ì†ì‹¤ ë° ì •í™•ë„ ê³„ì‚°
            train_loss = running_loss / len(train_loader)
            train_acc = correct / total * 100
            train_losses.append(train_loss)
            train_accuracies.append(train_acc)
        
            # ê²€ì¦ ì†ì‹¤ ë° ì •í™•ë„ ê³„ì‚°
            val_loss, val_acc = evaluate(model, val_loader)
            val_losses.append(val_loss)
            val_accuracies.append(val_acc)
        
            # í˜„ì¬ í•™ìŠµë¥  í™•ì¸
            current_lr = optimizer.param_groups[0]['lr']
        
            # ë¡œê·¸ ì¶œë ¥
            print(f"\n Epoch [{epoch+1}/{EPOCHS}]")
            print(f"Train Loss: {train_loss:.4f} | Train Acc: {train_acc:.2f}%")
            print(f"Val Loss: {val_loss:.4f} | Val Acc: {val_acc:.2f}%")
            print(f"Learning Rate: {current_lr:.6f}")
        
            # ì–¼ë¦¬ ìŠ¤íƒ‘ í™•ì¸
            if val_loss < best_val_loss - min_delta:
                best_val_loss = val_loss
                counter = 0
                torch.save(model.state_dict(), best_model_path)
                print(f"ì„±ëŠ¥ í–¥ìƒ! ëª¨ë¸ ì €ì¥ë¨: {best_model_path}")
            else:
                counter += 1
                print(f"ê°œì„  ì—†ìŒ (Counter: {counter}/{patience})")
        
            # ì–¼ë¦¬ ìŠ¤íƒ‘ ì¡°ê±´ ì¶©ì¡± ì‹œ í•™ìŠµ ì¤‘ë‹¨
            if counter >= patience:
                print(f"\nì–¼ë¦¬ ìŠ¤íƒ‘ ë°œìƒ! {epoch+1} ì—í¬í¬ì—ì„œ í•™ìŠµ ì¢…ë£Œ")
                break
        
            # í•™ìŠµë¥  ìŠ¤ì¼€ì¤„ëŸ¬ ì—…ë°ì´íŠ¸
            scheduler.step()
        ```
        
    
    ![ResNet50 ëª¨ë¸ í•™ìŠµ ì‹œê°í™”](attachment:e0fa896d-56ce-4661-af1b-7295c8ddb970:image.png)
    
    ResNet50 ëª¨ë¸ í•™ìŠµ ì‹œê°í™”
    
    ```python
    import matplotlib.pyplot as plt
    
    # ëª¨ë¸ í‰ê°€ í•¨ìˆ˜
    def evaluate(model, dataloader):
        model.eval()  # í‰ê°€ ëª¨ë“œ
        correct = 0
        total = 0
        running_loss = 0.0
    
        with torch.no_grad():  # ê·¸ë˜ë””ì–¸íŠ¸ ê³„ì‚° X (ì†ë„ ìµœì í™”)
            for images, labels in dataloader:
                images, labels = images.to(device), labels.to(device)
                outputs = model(images)
                loss = criterion(outputs, labels)
                running_loss += loss.item()
    
                _, predicted = torch.max(outputs, 1)
                total += labels.size(0)
                correct += (predicted == labels).sum().item()
    
        avg_loss = running_loss / len(dataloader)
        accuracy = correct / total * 100
        return avg_loss, accuracy
    
    # ìµœì¢… í…ŒìŠ¤íŠ¸ ë°ì´í„° í‰ê°€
    test_loss, test_acc = evaluate(model, test_loader)
    
    print(f"Test Loss: {test_loss:.4f}")
    print(f"Test Accuracy: {test_acc:.2f}%")
    
    # ê²°ê³¼ 
    Test Loss: 0.1191
    Test Accuracy: 97.79%
    ```
    
    - ì—­ì‹œë‚˜ `ResNet50` ëª¨ë¸ì€ ë§¤ìš° ë†’ì€ ì„±ëŠ¥ì„ ë³´ì—¬ì£¼ì—ˆë‹¤.
        
        â†’ ImageNet ëŒ€ê·œëª¨ ë°ì´í„°ì…‹ì—ì„œ ì‚¬ì „í•™ìŠµëœ ê°€ì¤‘ì¹˜ë¥¼ ì‚¬ìš©ì´ í° ì´ìœ ì´ë‹¤.
        
    - ì¶”ê°€ë¡œ **ê²½ëŸ‰í™”**ë¥¼ ì—¼ë‘í•˜ì—¬ `ResNet18` ëª¨ë¸ê³¼ ì—¬ëŸ¬ ëª¨ë¸ë“¤ì„ í•¨ê»˜ ëª¨ë¸í•™ìŠµì„ ì§„í–‰í•˜ì˜€ë‹¤.
        
        ![ì™¼ìª½ë¶€í„° ì°¨ë¡€ëŒ€ë¡œ VGG16, MobileNet, GoogLeNet, ResNet18 ëª¨ë¸](attachment:bf982158-3d06-4aa5-9c97-b5c24495fb5a:á„€á…¡á†¨_á„†á…©á„ƒá…¦á†¯á„‡á…§á†¯_á„€á…³á„…á…¢á„‘á…³(vgg16_mobilenetgooglenetresnet18).png)
        
        ì™¼ìª½ë¶€í„° ì°¨ë¡€ëŒ€ë¡œ VGG16, MobileNet, GoogLeNet, ResNet18 ëª¨ë¸
        
    
    ![ VGG16, MobileNet, GoogLeNet, ResNet18 ëª¨ë¸ì˜ ê° Train, Val, Test ì •í™•ë„](attachment:43ea6f17-8cdb-4277-9469-bd8c4e4044aa:á„€á…¡á†¨_á„†á…©á„ƒá…¦á†¯_test_acc.png)
    
     VGG16, MobileNet, GoogLeNet, ResNet18 ëª¨ë¸ì˜ ê° Train, Val, Test ì •í™•ë„
    
- ResNet50&18, VGG16 ê·¸ë¦¬ê³  MobileNet ëª¨ë¸ì„ ì‚¬ìš©ê²°ê³¼ ì‚¬ì „í•™ìŠµëœ ëª¨ë¸ì´ë‹¤ ë³´ë‹ˆ ì„±ëŠ¥ì´ ë§¤ìš° ì¢‹ì€ ë°©ë©´, ìƒëŒ€ì ìœ¼ë¡œ í•™ìŠµì„±ëŠ¥ì´ ë–¨ì–´ì§„ **GoogLeNet ëª¨ë¸ì„ ì ì°¨ ê°•í™”í•™ìŠµì„ ì§„í–‰**í•˜ëŠ” ë°©í–¥ìœ¼ë¡œ ì§„í–‰í•˜ì˜€ë‹¤.
- GoogLeNet ëª¨ë¸ì˜ íŠ¹ì§•
    1. **`Auxiliary Classifiers (ë³´ì¡° ë¶„ë¥˜ê¸°)`ê°€ ì¤‘ê°„ ë ˆì´ì–´ì— ì¡´ì¬**
        - í•™ìŠµ ì‹œ ë³´ì¡° ì†ì‹¤ë¡œë§Œ ì‚¬ìš©ë˜ë©° **ë³´ì¡° ë¶„ë¥˜ê¸°ë¡œ ì¸í•œ ê¸°ìš¸ê¸° ì†Œì‹¤ ì™„í™”**
        - **í•™ìŠµ ì†ë„ ë° ì•ˆì „ì„± í–¥ìƒ ê¸°ëŒ€**
    2. **ì‘ì€ ëª¨ë¸ í¬ê¸°**ë¡œë„ ì„±ëŠ¥ì´ ìš°ìˆ˜í•´ì„œ **ë¦¬ì†ŒìŠ¤ê°€ ì œí•œëœ í™˜ê²½**ì— ì í•©
    3. í•™ìŠµ ì•ˆì •ì„±ì„ ìœ„í•´ ë³´ì¡° ë¶„ë¥˜ê¸°ë¥¼ ì“°ëŠ” ì ì´ **ì†Œê·œëª¨ ë°ì´í„°ì…‹ í•™ìŠµì—ë„ ìœ ë¦¬**
        
        ![GoogLeNet ëª¨ë¸ êµ¬ì¡° ìš”ì•½ ê·¸ë¦¼](attachment:a06a8a56-48bf-43a7-b17d-ac5d1e399829:image.png)
        
        GoogLeNet ëª¨ë¸ êµ¬ì¡° ìš”ì•½ ê·¸ë¦¼
        

## 4. ì‹¤í—˜ ë°©ë²•

---

### **GoogLeNet ëª¨ë¸ ê¸°ë°˜ ì ì§„ì  ì„±ëŠ¥ ê°œì„  ì „ëµ**

- ê¸°ì¡´ ì¼ë°˜ GoogLeNet ëª¨ë¸ì—ì„œì˜ ì„±ëŠ¥ì´ ë‚®ì€ ì´ìœ ëŠ” ë‹¤ìŒê³¼ ê°™ë‹¤.
    1. ë³´ì¡° ë¶„ë¥˜ê¸°(aux) ë¹„í™œì„±í™” : `aux_logit=False`
    2. `CrossEntropyLoss` ì‚¬ìš©
    
    ```python
    	from torchvision import models
    import torch.nn as nn
    
    googlenet = models.googlenet(pretrained=True, aux_logits=False)
    googlenet.fc = nn.Linear(googlenet.fc.in_features, NUM_CLASSES)
    ```
    
- ë”°ë¼ì„œ ë³´ì¡°ë¶„ë¥˜ê¸°(aux) í™œì„±í™” ë° íŒŒì¸íŠœë‹ì„ ì§„í–‰í•˜ì˜€ë‹¤.
- ì¡°ê¸°ì¤‘ë‹¨(early stopping)ì„ ì ìš©í•œ í•´ë‹¹ íŠ¸ë ˆì´ë‹ í•¨ìˆ˜ë¥¼ ì„¤ì •í•˜ì—¬ ì§„í–‰í•˜ì˜€ë‹¤.
    
    ```python
    # ì–¼ë¦¬ ìŠ¤íƒ‘ í•™ìŠµ í•¨ìˆ˜
    def train_with_early_stopping(model, model_name, train_loader, val_loader, num_epochs=50, patience=5, min_delta=0.001):
        model.to(device)
    
        criterion = nn.CrossEntropyLoss()
        optimizer = optim.Adam(model.parameters(), lr=1e-4)
        scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=10, gamma=0.1)
    
        best_val_loss = np.inf
        counter = 0
        train_losses, val_losses = [], []
        train_accs, val_accs = [], []
    
        save_path = os.path.join(SAVE_DIR, f"{model_name}_best.pth")
    
        for epoch in range(num_epochs):
            model.train()
            running_loss = 0.0
            correct = 0
            total = 0
    
            for images, labels in train_loader:
                images, labels = images.to(device), labels.to(device)
                optimizer.zero_grad()
                outputs = model(images)
    
                # âœ… GoogLeNet ì²˜ë¦¬
                if isinstance(outputs, tuple) or hasattr(outputs, 'logits'):
                    outputs = outputs.logits
    
                loss = criterion(outputs, labels)
                loss.backward()
                optimizer.step()
    
                running_loss += loss.item()
                _, predicted = torch.max(outputs, 1)
                correct += (predicted == labels).sum().item()
                total += labels.size(0)
    
            train_loss = running_loss / len(train_loader)
            train_acc = correct / total * 100
            val_loss, val_acc = evaluate(model, val_loader, criterion)
    
            train_losses.append(train_loss)
            val_losses.append(val_loss)
            train_accs.append(train_acc)
            val_accs.append(val_acc)
    
            print(f"\nğŸ“˜ [{model_name.upper()}] Epoch [{epoch+1}/{num_epochs}]")
            print(f"   ğŸ”¹ Train Loss: {train_loss:.4f} | Train Acc: {train_acc:.2f}%")
            print(f"   ğŸ”¸ Val   Loss: {val_loss:.4f} | Val   Acc: {val_acc:.2f}%")
            print(f"   ğŸ“‰ LR: {optimizer.param_groups[0]['lr']:.6f}")
    
            # ì–¼ë¦¬ ìŠ¤íƒ‘ ì²´í¬
            if val_loss < best_val_loss - min_delta:
                best_val_loss = val_loss
                counter = 0
                torch.save(model.state_dict(), save_path)
                print(f"   âœ… ì„±ëŠ¥ í–¥ìƒ! ëª¨ë¸ ì €ì¥ë¨: {save_path}")
            else:
                counter += 1
                print(f"   â³ ê°œì„  ì—†ìŒ (Counter: {counter}/{patience})")
    
            if counter >= patience:
                print(f"\nğŸš¨ ì–¼ë¦¬ ìŠ¤íƒ‘ ë°œìƒ! {epoch+1} ì—í¬í¬ì—ì„œ í•™ìŠµ ì¢…ë£Œ ğŸš¨")
                break
    
            scheduler.step()
    
        return {
            "name": model_name,
            "train_loss": train_losses,
            "val_loss": val_losses,
            "train_acc": train_accs,
            "val_acc": val_accs
        }
    ```
    

---

### GoogLeNet ì„±ëŠ¥ ê°œì„ 

### 1. 1ì°¨ íŒŒì¸íŠœë‹ `(use_aux=True, freeze_base=False)`

```python
from torchvision import models
import torch.nn as nn
import torch

def get_finetuned_googlenet(num_classes, use_aux=True, freeze_base=False): #use_aux=Trueì¼ ê²½ìš° ë³´ì¡° ë¶„ë¥˜ê¸°ê¹Œì§€ í•¨ê»˜ í•™ìŠµ
    model = models.googlenet(pretrained=True, aux_logits=use_aux)

    # ë©”ì¸ ë¶„ë¥˜ê¸° êµì²´
    model.fc = nn.Linear(model.fc.in_features, num_classes)

    # ë³´ì¡° ë¶„ë¥˜ê¸°ë„ êµì²´ (aux_logits=Trueì¼ ë•Œë§Œ)
    if use_aux:
        model.aux1.fc2 = nn.Linear(model.aux1.fc2.in_features, num_classes)
        model.aux2.fc2 = nn.Linear(model.aux2.fc2.in_features, num_classes)

    # ë°±ë³¸ freeze (ì„ íƒ ì‹œ)
    if freeze_base:
        for name, param in model.named_parameters():
            if "fc" not in name and "aux" not in name:
                param.requires_grad = False

    return model
```

- aux1,aux2 ë¥¼ ì¶œë ¥ ë ˆì´ì„œ í´ë˜ìŠ¤ ìˆ˜ `num_classes` ì— ë§ì¶° ì¬ì •ì˜ í•˜ì˜€ìŒ
- **`freeze_base=False`** ë•ë¶„ì— ì‹¤ì œë¡œëŠ” ì´ë¯¸ ì „ì²´ íŒŒë¼ë¯¸í„°ê°€ `requires_grad=True`ì¸ ìƒíƒœì„
    
    â†’ ì•„ë¬´ ê²ƒë„ ì–¼ë¦¬ì§€(freeze) ì•Šì•˜ê¸° ë•Œë¬¸ì—, ì „ì²´ íŒŒë¼ë¯¸í„°ê°€ ê¸°ë³¸ì ìœ¼ë¡œ í•™ìŠµ ëŒ€ìƒì´ ëœë‹¤ëŠ” ëœ»
    
- ë”°ë¼ì„œ ìœ„ ì½”ë“œ í•¨ìˆ˜ëŠ” ìœ ì—°í•˜ê²Œ **ì „ì²´ íŒŒì¸íŠœë‹**ë„, **Gradual Unfreeze** ì´ˆê¸°ë„ ëª¨ë‘ ì»¤ë²„í•  ìˆ˜ ìˆëŠ” êµ¬ì¡°
    
    ```python
    # Fine-tuning 1ì°¨ (ì „ì²´ fine-tuning + ë³´ì¡° ë¶„ë¥˜ê¸° í¬í•¨)
    finetuned_googlenet = get_finetuned_googlenet(NUM_CLASSES, use_aux=True, freeze_base=False)
    
    # í•™ìŠµ ì‹¤í–‰
    googlenet_result = train_with_early_stopping(
        model=finetuned_googlenet,
        model_name="googlenet_finetuned",
        train_loader=train_loader,
        val_loader=val_loader,
        num_epochs=50,
        patience=5
    )
    ```
    

### 2. 2ì°¨ íŒŒì¸íŠœë‹ `(use_aux=True, freeze_base=True)`

```python
# Fine-tuning 2ì°¨ (Feature Extractor & Gradual Unfreeze ë°©ì‹ í•œë²ˆì—)

# Feature Extractor ë°©ì‹
model_fe = get_finetuned_googlenet(NUM_CLASSES, use_aux=True, freeze_base=True)
result_fe = train_with_early_stopping(
    model_fe,
    model_name="googlenet_feature_extractor",
    train_loader=train_loader,
    val_loader=val_loader,
    gradual_unfreeze=False  
)

# Gradual Unfreeze ë°©ì‹
model_gu = get_finetuned_googlenet(NUM_CLASSES, use_aux=True, freeze_base=True)
result_gu = train_with_early_stopping(
    model_gu,
    model_name="googlenet_gradual_unfreeze",
    train_loader=train_loader,
    val_loader=val_loader,
    gradual_unfreeze=True, 
    unfreeze_at=5
)
```

- í•œë²ˆì— ë‘ ëª¨ë¸ ìƒì„±í•˜ì—¬ ë‘ ì „ëµì„ ë™ì‹œì— ì§„í–‰í•˜ì˜€ë‹¤.
    - **Feature Extractor ë°©ì‹**
        - **ë°±ë³¸(Conv Layer)ì€ ê³ ì •(freeze)** â†’ `requires_grad = False`
        - **ë¶„ë¥˜ê¸°(FC + aux1, aux2)ë§Œ í•™ìŠµ**
        - ì¦‰, **ê¸°ì¡´ pretrained weightëŠ” ê·¸ëŒ€ë¡œ ìœ ì§€**í•˜ë©´ì„œ ìƒˆë¡œìš´ taskì— ë§ëŠ” **ì¶œë ¥ ë ˆì´ì–´ë§Œ í•™ìŠµ**
    - **Gradual Unfreeze**(ì ì§„ì  íŒŒë¼ë¯¸í„° í•´ì œ) **ë°©ì‹**
        - **ì²˜ìŒì—ëŠ” Feature Extractorì™€ ë™ì¼** â†’ `freeze_base=True`
        - **ì§€ì •ëœ epoch ì´í›„**, ë°±ë³¸ë„ requires_grad=Trueë¡œ ë³€ê²½ â†’ ì „ì²´ íŒŒì¸íŠœë‹ ì „í™˜
        - í•™ìŠµì´ **ì•ˆì •í™”ëœ í›„ ë°±ë³¸ì„ ì¡°ê¸ˆì”© í‘¸ëŠ” ì „ëµ**
    - ê° ëª¨ë¸ í•™ìŠµ í›„ csvíŒŒì¼ë¡œ ì €ì¥í•˜ì˜€ë‹¤.
        
        ```python
        # ë“œë¼ì´ë¸Œ ë‚´ ì €ì¥ ê²½ë¡œ
        CSV_SAVE_DIR = "/content/drive/MyDrive/Colab Notebooks/[á„á…¡á„á…¡á„‹á…© á„‡á…®á„á…³á„á…¢á†·á„‘á…³]/[á„á…¡á„á…¡á„‹á…© á„‡á…®á„á…³á„á…¢á†·á„‘á…³] á„€á…¢á„‹á…µá†«á„€á…ªá„Œá…¦2 á„‘á…©á†¯á„ƒá…¥"
        os.makedirs(CSV_SAVE_DIR, exist_ok=True)
        
        def save_result_to_csv_if_not_exists(result_dict, filename, save_dir):
            path = os.path.join(save_dir, filename)
            if os.path.exists(path):
                print(f"â© ì´ë¯¸ ì¡´ì¬í•¨: {path} â†’ ì €ì¥ ìƒëµ")
                return
            df = pd.DataFrame({
                "Epoch": list(range(1, len(result_dict["train_loss"]) + 1)),
                "Train Loss": result_dict["train_loss"],
                "Val Loss": result_dict["val_loss"],
                "Train Acc": result_dict["train_acc"],
                "Val Acc": result_dict["val_acc"],
            })
            df.to_csv(path, index=False)
            print(f"âœ… ì €ì¥ ì™„ë£Œ: {path}")
        
        save_result_to_csv_if_not_exists(googlenet_result, "googlenet_finetuned_result.csv", CSV_SAVE_DIR)
        ```
        
- ì „ì²´ ëª¨ë¸ ì‹œê°í™”
    - ê° ëª¨ë¸ì„ í•™ìŠµ ì‹œí‚¨ í›„ ì‹œê°í™”ë¥¼ ì§„í–‰í•˜ì˜€ìŒ
        
        ![image.png](attachment:9a17f588-6716-4dc5-b236-924568e89e1f:image.png)
        
        ![á„‰á…³á„á…³á„…á…µá†«á„‰á…£á†º 2025-03-30 á„‹á…©á„’á…® 8.10.35.png](attachment:8e23adc7-144a-4cc4-afc7-e211650fc827:á„‰á…³á„á…³á„…á…µá†«á„‰á…£á†º_2025-03-30_á„‹á…©á„’á…®_8.10.35.png)
        
    - íŒŒì¸íŠœë‹ì„ ì ìš©í•œ ëª¨ë¸ì´ ê¸°ë³¸ ëª¨ë¸(GoogLeNet_Pretrained) ë³´ë‹¤ í›¨ì”¬ ê°œì„ ëœ ê²ƒì´ ìˆì—ˆìœ¼ë©°, ë°˜ëŒ€ë¡œ ê°œì„ ì´ ì•ˆë˜ê³  ì˜¤íˆë ¤ í…ŒìŠ¤íŠ¸ ì •í™•ë„ê°€ ë–¨ì–´ì§„ ëª¨ë¸ì´ ë°œê²¬ ë˜ì—ˆë‹¤(**Feature Extractor).**
    - Feature Extractor ë°©ì‹ì—ì„œ ì„±ëŠ¥ì´ í•˜ë½í•œ ì´ìœ 
        
        Feature Extractor ë°©ì‹ì€ GoogLeNet ëª¨ë¸ì˜ ë°±ë³¸(Backbone)ì„ ê³ ì •(freeze)í•˜ê³ , ì¶œë ¥ì¸µ(fc, aux1, aux2)ë§Œì„ í•™ìŠµí•˜ëŠ” ë°©ì‹ì´ë‹¤. ì´ ì „ëµì€ ì¼ë°˜ì ìœ¼ë¡œ ë°ì´í„° ì–‘ì´ ì ê³ , ê¸°ì¡´ ì‚¬ì „í•™ìŠµëœ íŠ¹ì§•ì´ ìƒˆë¡œìš´ ë„ë©”ì¸ê³¼ ìœ ì‚¬í•  ë•Œ ìœ íš¨í•˜ë‹¤. ê·¸ëŸ¬ë‚˜ ë³¸ ì‹¤í—˜ì—ì„œëŠ” í•´ë‹¹ ë°©ì‹ì´ ì˜¤íˆë ¤ Pretrained GoogLeNetë³´ë‹¤ ë‚®ì€ ì •í™•ë„ë¥¼ ë³´ì˜€ë‹¤(Test Acc: 79.90% vs. 82.11%).
        
        â†’ ì´ëŸ¬í•œ ê²°ê³¼ëŠ” ë‹¤ìŒê³¼ ê°™ì€ ìš”ì¸ìœ¼ë¡œ ì„¤ëª…í•  ìˆ˜ ìˆë‹¤
        
        1. **í•™ìŠµ ê°€ëŠ¥í•œ íŒŒë¼ë¯¸í„°ì˜ ì œí•œ**
            
            Feature Extractor ë°©ì‹ì€ **ì¶œë ¥ì¸µ**ë§Œ í•™ìŠµí•˜ê¸° ë•Œë¬¸ì—, ë³‘í•´ì™€ ê°™ì€ íŠ¹ìˆ˜í•œ ì´ë¯¸ì§€ ë„ë©”ì¸ì— ëŒ€í•´ ì¶©ë¶„í•œ í‘œí˜„ í•™ìŠµì´ ì–´ë ¤ì›€ íŠ¹íˆ íŒ¨í„´ì´ ë¯¸ë¬˜í•˜ê²Œ ë³€í™”í•˜ëŠ” ê°ì ë³‘í•´ ì´ë¯¸ì§€ì— ëŒ€í•´ ì¼ë°˜ì ì¸ ë°±ë³¸ íŠ¹ì§•ë§Œìœ¼ë¡œëŠ” ì„±ëŠ¥ì´ ì œí•œë¨ â†’ ê³¼ì í•© ë°œìƒ
            
        2. **Pretrained ëª¨ë¸ì˜ ì¼ë°˜í™” ëŠ¥ë ¥**
            
            ì‚¬ì „í•™ìŠµëœ GoogLeNetì€ ImageNet ë°ì´í„°ì— ê¸°ë°˜í•˜ì—¬ ë‹¤ì–‘í•œ ì‹œê° íŠ¹ì§•ì„ í•™ìŠµí–ˆê¸° ë•Œë¬¸ì—, ê°„ë‹¨í•œ ë¶„ë¥˜ ë¬¸ì œì—ì„œëŠ” Feature Extractorë³´ë‹¤ ì˜¤íˆë ¤ ë” ì¼ë°˜ì ì¸ í‘œí˜„ ëŠ¥ë ¥ì„ ì œê³µí•  ìˆ˜ ìˆìŒ
            
        3. **ë°ì´í„°ì…‹ ë„ë©”ì¸ ê°„ ì°¨ì´ ë° ê·œëª¨ ì´ìŠˆ**
            
            ë³¸ í”„ë¡œì íŠ¸ì˜ ë°ì´í„°ì…‹ì€ í´ë˜ìŠ¤ ìˆ˜(3ê°œ)ê°€ ì ê³ , ì „ì²´ ì´ë¯¸ì§€ ìˆ˜ ë˜í•œ ëŒ€ê·œëª¨ê°€ ì•„ë‹˜ 
            
            ì´ëŸ¬í•œ ì¡°ê±´ì—ì„œëŠ” ì „ì²´ íŒŒë¼ë¯¸í„°ë¥¼ ì¡°ì •í•  ìˆ˜ ìˆëŠ” Fine-tuning ë˜ëŠ” Gradual Unfreeze ì „ëµì´ ë” íš¨ê³¼ì ìœ¼ë¡œ ë™ì‘í•¨
            
        
        ë”°ë¼ì„œ Feature Extractor ì „ëµì€ í•™ìŠµ ë²”ìœ„ê°€ ì œí•œì ì´ê¸° ë•Œë¬¸ì—, ë°ì´í„°ì˜ íŠ¹ì„±ê³¼ ë„ë©”ì¸ ê°„ ì°¨ì´ë¥¼ ì¶©ë¶„íˆ ë°˜ì˜í•˜ê¸° ì–´ë ¤ì› ìœ¼ë©°, ì´ëŠ” ì „ì²´ì ì¸ ë¶„ë¥˜ ì„±ëŠ¥ì˜ í•˜ë½ìœ¼ë¡œ ë³´ì„
        

### 3. Loss/Scheduler ë³€ê²½

- **Label Smoothing Loss ì ìš©**
    - Loss Functionì€ **ì •ë‹µì— ì–¼ë§ˆë‚˜ ê°€ê¹Œìš´ì§€ ìˆ˜ì¹˜í™” í•˜ëŠ” ê²ƒ**ì´ í•µì‹¬ì´ë‹¤.
        - `CrossEntropyLoss` : ì •ë‹µ ë ˆì´ë¸”ë§Œì„ ê¸°ì¤€ìœ¼ë¡œ í™•ì‹¤íˆ ë§ì¶°ì•¼ í•œë‹¤ê³  í•™ìŠµ
        - `LabelSmoothingLoss` : ì •ë‹µ ì´ì™¸ í´ë˜ìŠ¤ë„ ì•½ê°„ì˜ í™•ë¥ ì„ ë¶€ì—¬í•´ **ê³¼ì í•©ì„ ì¤„ì´ëŠ” ë°©í–¥**ìœ¼ë¡œ í•™ìŠµ
    - **í•µì‹¬ íŠ¹ì§•**
        - nn.CrossEntropyLoss(label_smoothing=0.1) ì ìš©
        - ëª¨ë¸ì˜ overconfidence ì™„í™”
        - ì‘ì€ ë°ì´í„°ì…‹ì— ìœ ë¦¬í•¨
    - ì½”ë“œ
        - LabelSmoothingLoss í†µí•© í•™ìŠµ í•¨ìˆ˜ ì •ì˜(ê¸°ë³¸ëª¨ë¸ + 1ì°¨ íŒŒì¸íŠœë‹)
            
            ```python
            def train_and_log(model, model_name, train_loader, val_loader, test_loader,
                              num_epochs=50, patience=5, min_delta=0.001,
                              loss_fn=None, collection_name=None):
                
                model = model.to(device)
                criterion = loss_fn if loss_fn else nn.CrossEntropyLoss()
                optimizer = optim.Adam(filter(lambda p: p.requires_grad, model.parameters()), lr=1e-4)
                scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=10, gamma=0.1)
                
                # âœ… MongoDB ì»¬ë ‰ì…˜ ì§€ì •
                collection = db[collection_name or model_name]
            
                # âœ… ì €ì¥ ê²½ë¡œ
                pth_path = os.path.join(BASE_DIR, f"{model_name}_best.pth")
                csv_path = os.path.join(BASE_DIR, f"{model_name}_result.csv")
                pt_path = os.path.join(BASE_DIR, f"{model_name}_probs.pt")
            
                best_val_loss = float("inf")
                counter = 0
                logs = []
            
                for epoch in range(num_epochs):
                    model.train()
                    start_time = time.strftime('%Y-%m-%d %H:%M:%S')
                    correct, total, running_loss = 0, 0, 0.0
            
                    for images, labels in train_loader:
                        images, labels = images.to(device), labels.to(device)
                        optimizer.zero_grad()
                        outputs = model(images)
                        if isinstance(outputs, tuple): outputs = outputs.logits
                        loss = criterion(outputs, labels)
                        loss.backward()
                        optimizer.step()
                        running_loss += loss.item()
                        _, preds = torch.max(outputs, 1)
                        correct += (preds == labels).sum().item()
                        total += labels.size(0)
            
                    train_loss = running_loss / len(train_loader)
                    train_acc = correct / total * 100
            
                    # ğŸ” ê²€ì¦
                    model.eval()
                    val_loss, val_correct, val_total, val_running_loss = 0, 0, 0, 0.0
                    with torch.no_grad():
                        for images, labels in val_loader:
                            images, labels = images.to(device), labels.to(device)
                            outputs = model(images)
                            if isinstance(outputs, tuple): outputs = outputs.logits
                            loss = criterion(outputs, labels)
                            val_running_loss += loss.item()
                            _, preds = torch.max(outputs, 1)
                            val_correct += (preds == labels).sum().item()
                            val_total += labels.size(0)
                    val_loss = val_running_loss / len(val_loader)
                    val_acc = val_correct / val_total * 100
                    end_time = time.strftime('%Y-%m-%d %H:%M:%S')
            
                    is_best = val_loss < best_val_loss - min_delta
                    if is_best:
                        best_val_loss = val_loss
                        counter = 0
                        torch.save(model.state_dict(), pth_path)
                    else:
                        counter += 1
                    scheduler.step()
            
                    # âœ… MongoDB ì €ì¥
                    log = {
                        "model": model_name,
                        "epoch": epoch + 1,
                        "start_time": start_time,
                        "end_time": end_time,
                        "train_loss": round(train_loss, 4),
                        "val_loss": round(val_loss, 4),
                        "train_acc": round(train_acc, 2),
                        "val_acc": round(val_acc, 2),
                        "is_best": is_best
                    }
                    collection.insert_one(log)
                    logs.append(log)
            
                    print(f"[{epoch+1:02d}] TrainAcc: {train_acc:.2f}% | ValAcc: {val_acc:.2f}% | {'âœ… Best' if is_best else 'â³'}")
                    if counter >= patience:
                        print("ğŸ›‘ Early Stopping!")
                        break
            
                # âœ… CSV ì €ì¥
                df = pd.DataFrame(logs)
                df.to_csv(csv_path, index=False)
            
                # âœ… Softmax ì €ì¥ (í…ŒìŠ¤íŠ¸ì…‹)
                model.load_state_dict(torch.load(pth_path))
                model.eval()
                probs_list, labels_list = [], []
                with torch.no_grad():
                    for images, labels in test_loader:
                        images = images.to(device)
                        outputs = model(images)
                        if isinstance(outputs, tuple): outputs = outputs.logits 
                        probs = F.softmax(outputs, dim=1)
                        probs_list.append(probs.cpu())
                        labels_list.append(labels)
                torch.save({"probs": torch.cat(probs_list), "labels": torch.cat(labels_list)}, pt_path)
                print(f"âœ… ì €ì¥ ì™„ë£Œ: {model_name} â†’ .pth / .csv / .pt")
            ```
            
            ```python
            from torch.nn import functional as F
            
            # ğŸ“Œ Label Smoothing Loss ì ìš©
            loss_fn = nn.CrossEntropyLoss(label_smoothing=0.1)
            
            # ğŸ”§ GoogLeNet base (Label Smoothing)
            googlenet_smooth = models.googlenet(pretrained=True, aux_logits=True) # êµ¬ì¡°ì ‡ìœ¼ë¡œëŠ” aux1, aux2 í¬í•¨
            googlenet_smooth.fc = nn.Linear(googlenet_smooth.fc.in_features, NUM_CLASSES)
            googlenet_smooth.aux1.fc2 = nn.Linear(googlenet_smooth.aux1.fc2.in_features, NUM_CLASSES) 
            googlenet_smooth.aux2.fc2 = nn.Linear(googlenet_smooth.aux2.fc2.in_features, NUM_CLASSES)
            """ 
            ë³´ì¡° ë¶„ë¥˜ê¸° ë¬´ì‹œí•  ê±°ë©´ ì™œ aux1,aux2ì— ëŒ€í•´ fc2ë¥¼ ì„¤ì •í–ˆëŠ”ê°€?
                > êµ¬ì¡°ì ìœ¼ë¡œ NUM_CLASSESì— ë§ê²Œ ë³´ì¡° ë¶„ë¥˜ê¸°ì˜ ì¶œë ¥ì¸µë§Œ ë§ì¶˜ê²ƒì„.
                í•™ìŠµí•¨ìˆ˜ `train_and_log()` ë‚´ë¶€ì—ì„œ `outputs = outputs.logits` ì²˜ë¦¬ë¥¼ í•˜ë¯€ë¡œ
                ì‹¤ì œë¡œ auxì¶œë ¥ì€ ì‚¬ìš©ì„ ì•ˆí•¨!!
            -> aux1,aux2ëŠ” êµ¬ì¡°ìƒ ì¡´ì¬ë§Œ í•˜ê³  í•™ìŠµ&ì¶œë ¥ì—ëŠ” ì˜í–¥ì„ ì•ˆë¼ì¹¨
            """
            
            # ğŸƒâ€â™‚ï¸ í•™ìŠµ ì‹œì‘
            train_and_log(
                model=googlenet_smooth,
                model_name="googlenet_smooth",
                train_loader=train_loader,
                val_loader=val_loader,
                test_loader=test_loader,
                loss_fn=loss_fn,
                collection_name="googlenet_smooth"  # âœ… ë³„ë„ ì»¬ë ‰ì…˜
            )
            ```
            
            ```python
            # âœ… 1ì°¨ íŒŒì¸íŠœë‹: ì „ì²´ í•™ìŠµ + ë³´ì¡° ë¶„ë¥˜ê¸°(aux) í¬í•¨
            googlenet_finetuned_smooth = models.googlenet(pretrained=True, aux_logits=True)
            googlenet_finetuned_smooth.fc = nn.Linear(googlenet_finetuned_smooth.fc.in_features, NUM_CLASSES)
            googlenet_finetuned_smooth.aux1.fc2 = nn.Linear(googlenet_finetuned_smooth.aux1.fc2.in_features, NUM_CLASSES)
            googlenet_finetuned_smooth.aux2.fc2 = nn.Linear(googlenet_finetuned_smooth.aux2.fc2.in_features, NUM_CLASSES)
            
            # ğŸ”¥ ì „ì²´ íŒŒë¼ë¯¸í„° í•™ìŠµ (requires_grad=True)
            for param in googlenet_finetuned_smooth.parameters():
                param.requires_grad = True
            
            # âœ… í•™ìŠµ ì‹¤í–‰
            train_and_log(
                model=googlenet_finetuned_smooth,
                model_name="googlenet_finetuned_smooth",
                train_loader=train_loader,
                val_loader=val_loader,
                test_loader=test_loader,
                loss_fn=loss_fn,  # Label Smoothing Loss
                collection_name="googlenet_finetuned_smooth"  # ë³„ë„ ì»¬ë ‰ì…˜
            )
            ```
            
        - LabelSmoothingLossí†µí•© í•™ìŠµ í•¨ìˆ˜ ì •ì˜ (Gradual Unfreeze _ 2ì°¨ íŒŒì¸íŠœë‹)
            
            ```python
            def train_and_log_with_unfreeze(model, model_name, train_loader, val_loader, test_loader,
                                            num_epochs=50, patience=5, min_delta=0.001,
                                            loss_fn=None, collection_name=None,
                                            gradual_unfreeze=False, unfreeze_at=5):
                model = model.to(device)
                criterion = loss_fn if loss_fn else nn.CrossEntropyLoss()
                """
                ì™¸ë¶€ì—ì„œ loss_fnì„ ë„˜ê²¨ì£¼ë©´ ê·¸ê²ƒì„ ì‚¬ìš©í•˜ê³ ,
                ë„˜ê²¨ì£¼ì§€ ì•Šìœ¼ë©´ ê¸°ë³¸ê°’ì¸ nn.CrossEntropyLoss()ë¥¼ ì‚¬ìš©í•œë‹¤.
                """
            
                optimizer = optim.Adam(filter(lambda p: p.requires_grad, model.parameters()), lr=1e-4)
                scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=10, gamma=0.1)
                collection = db[collection_name or model_name]
            
                pth_path = os.path.join(BASE_DIR, f"{model_name}_best.pth")
                csv_path = os.path.join(BASE_DIR, f"{model_name}_result.csv")
                pt_path = os.path.join(BASE_DIR, f"{model_name}_probs.pt")
            
                best_val_loss = float("inf")
                counter = 0
                logs = []
            
                for epoch in range(num_epochs):
                    model.train()
                    start_time = time.strftime('%Y-%m-%d %H:%M:%S')
            
                    # ğŸ”“ Gradual Unfreeze
                    if gradual_unfreeze and epoch == unfreeze_at:
                        print(f"ğŸ”“ Gradual Unfreeze ì‹œì‘ (Epoch {epoch})")
                        for param in model.parameters():
                            param.requires_grad = True
                        optimizer = optim.Adam(model.parameters(), lr=1e-5)
            
                    running_loss, correct, total = 0.0, 0, 0
                    for images, labels in train_loader:
                        images, labels = images.to(device), labels.to(device)
                        optimizer.zero_grad()
                        outputs = model(images)
                        if isinstance(outputs, tuple): outputs = outputs.logits
                        loss = criterion(outputs, labels)
                        loss.backward()
                        optimizer.step()
                        running_loss += loss.item()
                        _, preds = torch.max(outputs, 1)
                        correct += (preds == labels).sum().item()
                        total += labels.size(0)
            
                    train_loss = running_loss / len(train_loader)
                    train_acc = correct / total * 100
            
                    # ğŸ” ê²€ì¦
                    val_loss, val_correct, val_total, val_running_loss = 0, 0, 0, 0.0
                    model.eval()
                    with torch.no_grad():
                        for images, labels in val_loader:
                            images, labels = images.to(device), labels.to(device)
                            outputs = model(images)
                            if isinstance(outputs, tuple): outputs = outputs.logits
                            loss = criterion(outputs, labels)
                            val_running_loss += loss.item()
                            _, preds = torch.max(outputs, 1)
                            val_correct += (preds == labels).sum().item()
                            val_total += labels.size(0)
                    val_loss = val_running_loss / len(val_loader)
                    val_acc = val_correct / val_total * 100
                    end_time = time.strftime('%Y-%m-%d %H:%M:%S')
            
                    is_best = val_loss < best_val_loss - min_delta
                    if is_best:
                        best_val_loss = val_loss
                        counter = 0
                        torch.save(model.state_dict(), pth_path)
                    else:
                        counter += 1
                    scheduler.step()
            
                    # âœ… MongoDB ì €ì¥
                    log = {
                        "model": model_name, "epoch": epoch+1,
                        "start_time": start_time, "end_time": end_time,
                        "train_loss": round(train_loss, 4), "val_loss": round(val_loss, 4),
                        "train_acc": round(train_acc, 2), "val_acc": round(val_acc, 2),
                        "is_best": is_best
                    }
                    collection.insert_one(log)
                    logs.append(log)
            
                    print(f"[{epoch+1:02d}] TrainAcc: {train_acc:.2f}% | ValAcc: {val_acc:.2f}% | {'âœ… Best' if is_best else 'â³'}")
                    if counter >= patience:
                        print("ğŸ›‘ Early Stopping!")
                        break
            
                # âœ… CSV ì €ì¥
                pd.DataFrame(logs).to_csv(csv_path, index=False)
            
                # âœ… Softmax ì €ì¥
                model.load_state_dict(torch.load(pth_path))
                model.eval()
                probs_list, labels_list = [], []
                with torch.no_grad():
                    for images, labels in test_loader:
                        images = images.to(device)
                        outputs = model(images)
                        if isinstance(outputs, tuple): outputs = outputs.logits
                        probs = F.softmax(outputs, dim=1)
                        probs_list.append(probs.cpu())
                        labels_list.append(labels)
                torch.save({"probs": torch.cat(probs_list), "labels": torch.cat(labels_list)}, pt_path)
                print(f"âœ… ì €ì¥ ì™„ë£Œ: {model_name} â†’ .pth / .csv / .pt")
            ```
            
            ```python
            from torchvision import models
            
            # âœ… 2ì°¨ íŒŒì¸íŠœë‹: Gradual Unfreeze
            googlenet_gu_smooth = models.googlenet(pretrained=True, aux_logits=True)
            googlenet_gu_smooth.fc = nn.Linear(googlenet_gu_smooth.fc.in_features, NUM_CLASSES)
            googlenet_gu_smooth.aux1.fc2 = nn.Linear(googlenet_gu_smooth.aux1.fc2.in_features, NUM_CLASSES)
            googlenet_gu_smooth.aux2.fc2 = nn.Linear(googlenet_gu_smooth.aux2.fc2.in_features, NUM_CLASSES)
            
            # ğŸ”’ Conv ë ˆì´ì–´ëŠ” ì´ˆê¸° freeze, fc/auxë§Œ í•™ìŠµ
            for name, param in googlenet_gu_smooth.named_parameters():
                if "fc" in name or "aux" in name:
                    param.requires_grad = True
                else:
                    param.requires_grad = False
            
            # ğŸ”§ ì†ì‹¤í•¨ìˆ˜ ì •ì˜ (í•¨ìˆ˜ ë°”ê¹¥)
            loss_fn = nn.CrossEntropyLoss(label_smoothing=0.1) 
            # loss_fnì •ì˜ê°€ í•¨ìˆ˜ ë°”ê¹¥ì— ì¡´ì¬í•´ì•¼í•¨ 
            # train_and_log_with_unfreeze() í•¨ìˆ˜ ì•ˆì—ì„œ ë‹¤ì‹œ ì •ì˜í•˜ë©´ ì•ˆë¨
            # í•¨ìˆ˜ ë‚´ë¶€ì—ì„œ loss_fnì„ ë‹¤ì‹œ ì •ì˜
            
            # âœ… Gradual Unfreeze í•™ìŠµ ì‹¤í–‰
            train_and_log_with_unfreeze(
                model=googlenet_gu_smooth,
                model_name="googlenet_gu_smooth",
                train_loader=train_loader,
                val_loader=val_loader,
                test_loader=test_loader,
                loss_fn=loss_fn,  # âœ… Label Smoothing ì ìš©
                collection_name="googlenet_gu_smooth",  # MongoDB ì»¬ë ‰ì…˜ëª…
                gradual_unfreeze=True,  # âœ… ì ì§„ì  Unfreeze ì‚¬ìš©
                unfreeze_at=5           # ğŸ”“ 5 epoch í›„ ì „ì²´ layer í™œì„±í™”
            )
            ```
            
    - ì•„ë˜ ê²°ê³¼ëŠ” `Label Smoothing Loss` ****ë¥¼ ì ìš©í•˜ê³  ë‚œ í›„ í…ŒìŠ¤íŠ¸ ì •í™•ë„ ì´ë‹¤.
        
        ![á„‰á…³á„á…³á„…á…µá†«á„‰á…£á†º 2025-03-30 á„‹á…©á„’á…® 9.35.49.png](attachment:575d92e0-9db6-4fa1-b460-6bc973719e71:á„‰á…³á„á…³á„…á…µá†«á„‰á…£á†º_2025-03-30_á„‹á…©á„’á…®_9.35.49.png)
        
    - ì•„ë˜ ë§‰ëŒ€ ê·¸ë˜í”„ëŠ” `Label Smoothing Loss`  ìœ ë¬´ì— ë”°ë¥¸ ì •í™•ë„ ë¹„êµë¥¼ ìœ„í•´ ì‹œê°í™”ë¥¼ ì§„í–‰í–ˆë‹¤.(_*smooth*_ê°€ `Label Smoothing Loss`ì ìš©í•œ ëª¨ë¸)
        
        ![image.png](attachment:5b08981f-537e-4ca0-81fd-7f45fab10921:image.png)
        
    - ì›ë˜ ì˜ë„ëŠ” Label Smoothingì„ ë„ì…í•˜ì—¬ ëª¨ë¸ì´ **ê³¼ì í•©ì„ ë°©ì§€í•˜ê³  ì¼ë°˜í™” ì„±ëŠ¥ì„ ë†’ì´ë„ë¡** ê°œì„ í•˜ë ¤ë˜ ê²ƒì´ì—ˆëŠ”ë°, ì˜¤íˆë ¤ 2ì°¨ íŒŒì¸íŠœë‹(gradual unfreeze)ì—ì„œëŠ” ì •í™•ë„ê°€ **ë‚®ì•„ì§€ëŠ”** ê²°ê³¼ê°€ ë‚˜ì™”ë‹¤.
    - Gradual Unfreeze & Smoothing ì¡°í•©ì´ ì•ˆì¢‹ì€ ì´ìœ 
        
        
        | **êµ¬ë¶„** | **Gradual Unfreeze** | **Label Smoothing** |
        | --- | --- | --- |
        | ëª©ì  | feature ë³´ì¡´ + ì•ˆì •ì  ë¯¸ì„¸ì¡°ì • | ê³¼ì í•© ë°©ì§€ + regularization |
        | íŠ¹ì§• | ì ì§„ì ìœ¼ë¡œ í•™ìŠµ ê°•ë„ â†‘ | í•™ìŠµ ê°•ë„ë¥¼ â†“ì‹œí‚¤ëŠ” ê²½í–¥ |
        | ì¶©ëŒ | í•™ìŠµì˜ **ë¯¼ê°í•œ ì´ˆê¸° ë‹¨ê³„ì—ì„œ**, í•™ìŠµ ëŒ€ìƒì´ ë„ˆë¬´ softí•´ì ¸ â†’ ìœ ì˜ë¯¸í•œ íŒŒë¼ë¯¸í„° ì—…ë°ì´íŠ¸ê°€ ì–´ë ¤ì›€ |  |
        
        â†’ Gradual Unfreeze ëŠ” ì ì§„ì ì¸ í•™ìŠµì´ í•„ìš”í•œë° **Label Smoothingì€ í•™ìŠµ ìì²´ë¥¼ ë” ì•½í•˜ê²Œ ë§Œë“¤ê¸° ë•Œë¬¸ì—** íŒŒë¼ë¯¸í„°ê°€ í’€ë¦° í›„ì—ë„ ëª¨ë¸ì´ ì œëŒ€ë¡œ í•™ìŠµí•˜ì§€ ëª»í•œê²ƒìœ¼ë¡œ í™•ì¸ë¨
        
- **Label Smoothing + ReduceLROnPlateau ìŠ¤ì¼€ì¤„ëŸ¬ ì ìš©**
    - **Reduce Learning Rate On Plateau :** í•™ìŠµì´ í‰íƒ„(plateu) ìƒíƒœì— ë„ë‹¬í–ˆì„ ë•Œ, í•™ìŠµë¥ (Learning Rate)ë¥¼ ìë™ìœ¼ë¡œ ì¤„ì—¬ì£¼ëŠ” ìŠ¤ì¼€ì¤„ëŸ¬
    - **í•µì‹¬ íŠ¹ì§•**
        - ì¼ì • epoch ë™ì•ˆ val_loss ê°œì„  ì—†ì„ ì‹œ learning rate ê°ì†Œ
        - StepLRë³´ë‹¤ ë” ë¶€ë“œëŸ½ê³  ì •ë°€í•œ ì¡°ì ˆ
        - min_delta, factor, patience ì¡°ì • ê°€ëŠ¥
    
    ```python
    scheduler = ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=2)
    ```
    
    - ê²€ì¦ ì†ì‹¤ì´ 2í­ë™ì•ˆ ê°œì„  ë˜ì§€ ì•Šìœ¼ë©´, í•™ìŠµë¥ ì„ í˜„ì¬ì˜ 50%ë¡œ ê°ì†Œì‹œí‚¨ë‹¤ëŠ” ëœ»
    - Label Smoothingì€ ì¼ë°˜ì ìœ¼ë¡œ í•™ìŠµì„ ë¶€ë“œëŸ½ê²Œ ìœ ë„í•˜ê¸°ì— ê³ ì •ì ì¸ StepLRë³´ë‹¤ **ë™ì ìœ¼ë¡œ ë°˜ì‘í•˜ëŠ” ResuceLROnPlateuë¡œ ì§„í–‰**
    - ì½”ë“œ
        - googlenet_smooth_rlrop (ê¸°ì¡´ êµ¬ê¸€ë„· ëª¨ë¸ + Smoothing + ReduceLROnPlateau)
            
            ```python
            def train_and_log(model, model_name, train_loader, val_loader, test_loader,
                              num_epochs=50, patience=5, min_delta=0.001,
                              loss_fn=None, collection_name=None):
                
                model = model.to(device)
                criterion = loss_fn if loss_fn else nn.CrossEntropyLoss()
                optimizer = optim.Adam(filter(lambda p: p.requires_grad, model.parameters()), lr=1e-4)
                
                # âœ… ReduceLROnPlateau ìŠ¤ì¼€ì¤„ëŸ¬ ì ìš©
                scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min',
                                                                 factor=0.5, patience=2,
                                                                 verbose=True, min_lr=1e-6)
            
                collection = db[collection_name or model_name]
                pth_path = os.path.join(BASE_DIR, f"{model_name}_best.pth")
                csv_path = os.path.join(BASE_DIR, f"{model_name}_result.csv")
                pt_path = os.path.join(BASE_DIR, f"{model_name}_probs.pt")
            
                best_val_loss = float("inf")
                counter = 0
                logs = []
            
                for epoch in range(num_epochs):
                    model.train()
                    start_time = time.strftime('%Y-%m-%d %H:%M:%S')
                    running_loss, correct, total = 0.0, 0, 0
            
                    for images, labels in train_loader:
                        images, labels = images.to(device), labels.to(device)
                        optimizer.zero_grad()
                        outputs = model(images)
                        if isinstance(outputs, tuple): outputs = outputs.logits
                        loss = criterion(outputs, labels)
                        loss.backward()
                        optimizer.step()
                        running_loss += loss.item()
                        _, preds = torch.max(outputs, 1)
                        correct += (preds == labels).sum().item()
                        total += labels.size(0)
            
                    train_loss = running_loss / len(train_loader)
                    train_acc = correct / total * 100
            
                    # ğŸ” ê²€ì¦
                    model.eval()
                    val_loss, val_correct, val_total, val_running_loss = 0, 0, 0, 0.0
                    with torch.no_grad():
                        for images, labels in val_loader:
                            images, labels = images.to(device), labels.to(device)
                            outputs = model(images)
                            if isinstance(outputs, tuple): outputs = outputs.logits
                            loss = criterion(outputs, labels)
                            val_running_loss += loss.item()
                            _, preds = torch.max(outputs, 1)
                            val_correct += (preds == labels).sum().item()
                            val_total += labels.size(0)
                    val_loss = val_running_loss / len(val_loader)
                    val_acc = val_correct / val_total * 100
                    end_time = time.strftime('%Y-%m-%d %H:%M:%S')
            
                    # ğŸ”½ ìŠ¤ì¼€ì¤„ëŸ¬ ì—…ë°ì´íŠ¸
                    scheduler.step(val_loss)
            
                    is_best = val_loss < best_val_loss - min_delta
                    if is_best:
                        best_val_loss = val_loss
                        counter = 0
                        torch.save(model.state_dict(), pth_path)
                    else:
                        counter += 1
            
                    # âœ… MongoDB ì €ì¥
                    log = {
                        "model": model_name,
                        "epoch": epoch + 1,
                        "start_time": start_time,
                        "end_time": end_time,
                        "train_loss": round(train_loss, 4),
                        "val_loss": round(val_loss, 4),
                        "train_acc": round(train_acc, 2),
                        "val_acc": round(val_acc, 2),
                        "is_best": is_best
                    }
                    collection.insert_one(log)
                    logs.append(log)
            
                    print(f"[{epoch+1:02d}] TrainAcc: {train_acc:.2f}% | ValAcc: {val_acc:.2f}% | {'âœ… Best' if is_best else 'â³'}")
                    if counter >= patience:
                        print("ğŸ›‘ Early Stopping!")
                        break
            
                # âœ… CSV ì €ì¥
                pd.DataFrame(logs).to_csv(csv_path, index=False)
            
                # âœ… Softmax ì €ì¥
                model.load_state_dict(torch.load(pth_path))
                model.eval()
                probs_list, labels_list = [], []
                with torch.no_grad():
                    for images, labels in test_loader:
                        images = images.to(device)
                        outputs = model(images)
                        if isinstance(outputs, tuple): outputs = outputs.logits
                        probs = F.softmax(outputs, dim=1)
                        probs_list.append(probs.cpu())
                        labels_list.append(labels)
                torch.save({"probs": torch.cat(probs_list), "labels": torch.cat(labels_list)}, pt_path)
                print(f"âœ… ì €ì¥ ì™„ë£Œ: {model_name} â†’ .pth / .csv / .pt")
            ```
            
            ```python
            from torchvision import models
            import torch.nn as nn
            
            # âœ… ì†ì‹¤ í•¨ìˆ˜: Label Smoothing
            loss_fn = nn.CrossEntropyLoss(label_smoothing=0.1)
            
            # âœ… GoogLeNet base + Label Smoothing + ReduceLROnPlateau
            googlenet_smooth_rlrop = models.googlenet(pretrained=True, aux_logits=True)
            googlenet_smooth_rlrop.fc = nn.Linear(googlenet_smooth_rlrop.fc.in_features, NUM_CLASSES)
            googlenet_smooth_rlrop.aux1.fc2 = nn.Linear(googlenet_smooth_rlrop.aux1.fc2.in_features, NUM_CLASSES)
            googlenet_smooth_rlrop.aux2.fc2 = nn.Linear(googlenet_smooth_rlrop.aux2.fc2.in_features, NUM_CLASSES)
            
            # âœ… ì „ì²´ íŒŒë¼ë¯¸í„° í•™ìŠµ ê°€ëŠ¥í•˜ë„ë¡ ì„¤ì •
            for param in googlenet_smooth_rlrop.parameters():
                param.requires_grad = True
            
            # âœ… í•™ìŠµ ì‹œì‘
            train_and_log(
                model=googlenet_smooth_rlrop,
                model_name="googlenet_smooth_rlrop",
                train_loader=train_loader,
                val_loader=val_loader,
                test_loader=test_loader,
                loss_fn=loss_fn,
                collection_name="googlenet_smooth_rlrop"
            )
            ```
            
            ```python
            from torchvision import models
            import torch.nn as nn
            
            # âœ… ì†ì‹¤ í•¨ìˆ˜: Label Smoothing ìœ ì§€
            loss_fn = nn.CrossEntropyLoss(label_smoothing=0.1)
            
            # âœ… GoogLeNet + 1ì°¨ íŒŒì¸íŠœë‹ (ReduceLROnPlateau í¬í•¨)
            googlenet_finetuned_smooth_rlrop = models.googlenet(pretrained=True, aux_logits=True)
            googlenet_finetuned_smooth_rlrop.fc = nn.Linear(googlenet_finetuned_smooth_rlrop.fc.in_features, NUM_CLASSES)
            googlenet_finetuned_smooth_rlrop.aux1.fc2 = nn.Linear(googlenet_finetuned_smooth_rlrop.aux1.fc2.in_features, NUM_CLASSES)
            googlenet_finetuned_smooth_rlrop.aux2.fc2 = nn.Linear(googlenet_finetuned_smooth_rlrop.aux2.fc2.in_features, NUM_CLASSES)
            
            # âœ… ì „ì²´ íŒŒë¼ë¯¸í„° í•™ìŠµ ê°€ëŠ¥í•˜ë„ë¡ ì„¤ì •
            for param in googlenet_finetuned_smooth_rlrop.parameters():
                param.requires_grad = True
            
            # âœ… í•™ìŠµ ì‹¤í–‰
            train_and_log(
                model=googlenet_finetuned_smooth_rlrop,
                model_name="googlenet_finetuned_smooth_rlrop",
                train_loader=train_loader,
                val_loader=val_loader,
                test_loader=test_loader,
                loss_fn=loss_fn,
                collection_name="googlenet_finetuned_smooth_rlrop"
            )
            ```
            
        - GoogLeNet + Gradual Unfreeze + Label Smoothing + ReduceLROnPlateau í†µí•© ì…€
            
            ```python
            def train_and_log_with_unfreeze_rlrop(model, model_name, train_loader, val_loader, test_loader,
                                                  num_epochs=50, patience=5, min_delta=0.001,
                                                  loss_fn=None, collection_name=None,
                                                  gradual_unfreeze=False, unfreeze_at=5):
                model = model.to(device)
                criterion = loss_fn if loss_fn else nn.CrossEntropyLoss()
            
                optimizer = optim.Adam(filter(lambda p: p.requires_grad, model.parameters()), lr=1e-4)
            
                # ReduceLROnPlateau ìŠ¤ì¼€ì¤„ëŸ¬ ì ìš©
                scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min',
                                                                 factor=0.5, patience=2,
                                                                 verbose=True, min_lr=1e-6)
            
                collection = db[collection_name or model_name]
                pth_path = os.path.join(BASE_DIR, f"{model_name}_best.pth")
                csv_path = os.path.join(BASE_DIR, f"{model_name}_result.csv")
                pt_path = os.path.join(BASE_DIR, f"{model_name}_probs.pt")
            
                best_val_loss = float("inf")
                counter = 0
                logs = []
            
                for epoch in range(num_epochs):
                    model.train()
                    start_time = time.strftime('%Y-%m-%d %H:%M:%S')
            
                    # Gradual Unfreeze
                    if gradual_unfreeze and epoch == unfreeze_at:
                        print(f"Gradual Unfreeze ì‹œì‘ (Epoch {epoch})")
                        for param in model.parameters():
                            param.requires_grad = True
                        optimizer = optim.Adam(model.parameters(), lr=1e-5)
            
                    running_loss, correct, total = 0.0, 0, 0
                    for images, labels in train_loader:
                        images, labels = images.to(device), labels.to(device)
                        optimizer.zero_grad()
                        outputs = model(images)
                        if isinstance(outputs, tuple): outputs = outputs.logits
                        loss = criterion(outputs, labels)
                        loss.backward()
                        optimizer.step()
                        running_loss += loss.item()
                        _, preds = torch.max(outputs, 1)
                        correct += (preds == labels).sum().item()
                        total += labels.size(0)
            
                    train_loss = running_loss / len(train_loader)
                    train_acc = correct / total * 100
            
                    # ê²€ì¦
                    model.eval()
                    val_loss, val_correct, val_total, val_running_loss = 0, 0, 0, 0.0
                    with torch.no_grad():
                        for images, labels in val_loader:
                            images, labels = images.to(device), labels.to(device)
                            outputs = model(images)
                            if isinstance(outputs, tuple): outputs = outputs.logits
                            loss = criterion(outputs, labels)
                            val_running_loss += loss.item()
                            _, preds = torch.max(outputs, 1)
                            val_correct += (preds == labels).sum().item()
                            val_total += labels.size(0)
                    val_loss = val_running_loss / len(val_loader)
                    val_acc = val_correct / val_total * 100
                    end_time = time.strftime('%Y-%m-%d %H:%M:%S')
            
                    # ìŠ¤ì¼€ì¤„ëŸ¬ ì ìš©
                    scheduler.step(val_loss)
            
                    is_best = val_loss < best_val_loss - min_delta
                    if is_best:
                        best_val_loss = val_loss
                        counter = 0
                        torch.save(model.state_dict(), pth_path)
                    else:
                        counter += 1
            
                    # MongoDB ì €ì¥
                    log = {
                        "model": model_name, "epoch": epoch + 1,
                        "start_time": start_time, "end_time": end_time,
                        "train_loss": round(train_loss, 4), "val_loss": round(val_loss, 4),
                        "train_acc": round(train_acc, 2), "val_acc": round(val_acc, 2),
                        "is_best": is_best
                    }
                    collection.insert_one(log)
                    logs.append(log)
            
                    print(f"[{epoch+1:02d}] TrainAcc: {train_acc:.2f}% | ValAcc: {val_acc:.2f}% | {'âœ… Best' if is_best else 'â³'}")
                    if counter >= patience:
                        print("Early Stopping!")
                        break
            
                # CSV ì €ì¥
                pd.DataFrame(logs).to_csv(csv_path, index=False)
            
                # Softmax ì €ì¥
                model.load_state_dict(torch.load(pth_path))
                model.eval()
                probs_list, labels_list = [], []
                with torch.no_grad():
                    for images, labels in test_loader:
                        images = images.to(device)
                        outputs = model(images)
                        if isinstance(outputs, tuple): outputs = outputs.logits
                        probs = F.softmax(outputs, dim=1)
                        probs_list.append(probs.cpu())
                        labels_list.append(labels)
                torch.save({"probs": torch.cat(probs_list), "labels": torch.cat(labels_list)}, pt_path)
                print(f"âœ… ì €ì¥ ì™„ë£Œ: {model_name} â†’ .pth / .csv / .pt")
            ```
            
            ```python
            from torchvision import models
            import torch.nn as nn
            
            # Label Smoothing ì†ì‹¤ í•¨ìˆ˜
            loss_fn = nn.CrossEntropyLoss(label_smoothing=0.1)
            
            # GoogLeNet + Gradual Unfreeze + LabelSmoothing + ReduceLROnPlateau
            googlenet_gu_smooth_rlrop = models.googlenet(pretrained=True, aux_logits=True)
            googlenet_gu_smooth_rlrop.fc = nn.Linear(googlenet_gu_smooth_rlrop.fc.in_features, NUM_CLASSES)
            googlenet_gu_smooth_rlrop.aux1.fc2 = nn.Linear(googlenet_gu_smooth_rlrop.aux1.fc2.in_features, NUM_CLASSES)
            googlenet_gu_smooth_rlrop.aux2.fc2 = nn.Linear(googlenet_gu_smooth_rlrop.aux2.fc2.in_features, NUM_CLASSES)
            
            # Conv ë ˆì´ì–´ ì´ˆê¸° freeze
            for name, param in googlenet_gu_smooth_rlrop.named_parameters():
                if "fc" in name or "aux" in name:
                    param.requires_grad = True
                else:
                    param.requires_grad = False
            
            train_and_log_with_unfreeze_rlrop(
                model=googlenet_gu_smooth_rlrop,
                model_name="googlenet_gu_smooth_rlrop",
                train_loader=train_loader,
                val_loader=val_loader,
                test_loader=test_loader,
                loss_fn=loss_fn,
                collection_name="googlenet_gu_smooth_rlrop",  # MongoDB ë³„ë„ ì»¬ë ‰ì…˜
                gradual_unfreeze=True,  # ì ì§„ì  íŒŒë¼ë¯¸í„° í•´ì œ
                unfreeze_at=5           # 5 epoch í›„ ì „ì²´ unfrozen
            )
            ```
            
        - í…ŒìŠ¤íŠ¸ ì •í™•ë„ í‰ê°€ ë° MongoDBì— ì €ì¥
            
            ```python
            from pymongo import MongoClient
            import certifi
            
            # âœ… MongoDB ì—°ê²° (ì´ë¯¸ ì—°ê²°ëœ ê²½ìš° ìƒëµ ê°€ëŠ¥)
            MONGO_URI = "mongodb+srv://dnjsgh1820:dkf1ckrp@cluster-ktb.cmcdvg7.mongodb.net/"
            client = MongoClient(MONGO_URI, tlsCAFile=certifi.where())
            db = client["model_logs"]
            
            # âœ… í…ŒìŠ¤íŠ¸ ì •í™•ë„ ê³„ì‚° í•¨ìˆ˜
            def evaluate_test_accuracy(model, dataloader):
                model.eval()
                correct, total = 0, 0
                with torch.no_grad():
                    for images, labels in dataloader:
                        images, labels = images.to(device), labels.to(device)
                        outputs = model(images)
                        if isinstance(outputs, tuple): outputs = outputs.logits
                        _, preds = torch.max(outputs, 1)
                        correct += (preds == labels).sum().item()
                        total += labels.size(0)
                return round(correct / total * 100, 2)
            
            # âœ… MongoDBì— ì €ì¥ í•¨ìˆ˜
            def log_test_accuracy_to_mongodb(model_name, model, test_loader):
                # ë¨¼ì € í…ŒìŠ¤íŠ¸ ì •í™•ë„ ê³„ì‚°
                test_acc = evaluate_test_accuracy(model, test_loader)
            
                # âœ… ì´ë¯¸ ê°™ì€ type='final_test' ë¡œê·¸ê°€ ìˆëŠ”ì§€ í™•ì¸
                collection = db[model_name]
                existing = collection.find_one({"type": "final_test"})
                
                if existing:
                    print(f"âš ï¸ {model_name} ì´ë¯¸ í…ŒìŠ¤íŠ¸ ì •í™•ë„ê°€ ì €ì¥ë˜ì–´ ìˆìŒ â†’ ë®ì–´ì“°ê¸° ìƒëµ")
                    print(f"ğŸ“Œ {model_name} í…ŒìŠ¤íŠ¸ ì •í™•ë„: {test_acc}% (MongoDB ì €ì¥ ì™„ë£Œ)")
                    return  # ì¤‘ë³µ ë°©ì§€ìš© ì¢…ë£Œ
            
                # ìƒˆë¡œ ì €ì¥
                result = {
                    "model": model_name,
                    "test_accuracy": test_acc,
                    "type": "final_test"
                }
                collection.insert_one(result)
                print(f"ğŸ“Œ {model_name} í…ŒìŠ¤íŠ¸ ì •í™•ë„: {test_acc}% (MongoDB ì €ì¥ ì™„ë£Œ)")
            ```
            
            ```python
            model_names = [
                "googlenet_smooth_rlrop",
                "googlenet_finetuned_smooth_rlrop",
                "googlenet_gu_smooth_rlrop"  # âœ… ìƒˆë¡œ ì¶”ê°€ëœ ëª¨ë¸
            ]
            
            # âœ… ê³µí†µ í•¨ìˆ˜: ëª¨ë¸ ë¡œë“œ
            def load_trained_model(model_name, weights_path, num_classes):
                model = models.googlenet(pretrained=False, aux_logits=True)
                model.fc = nn.Linear(model.fc.in_features, num_classes)
                model.aux1.fc2 = nn.Linear(model.aux1.fc2.in_features, num_classes)
                model.aux2.fc2 = nn.Linear(model.aux2.fc2.in_features, num_classes)
                model.load_state_dict(torch.load(weights_path, map_location=device))
                model.to(device)
                model.eval()
                return model
            
            # âœ… ì •í™•ë„ ì¸¡ì • ë° MongoDB ì €ì¥
            for model_name in model_names:
                weights_path = os.path.join(BASE_DIR, f"{model_name}_best.pth")
                model = load_trained_model(model_name, weights_path, NUM_CLASSES)
                log_test_accuracy_to_mongodb(model_name, model, test_loader)
            ```
            
            ```python
            
            # 3ê·¸ë£¹ ë¶„ë¥˜ ì‹œê°í™” ì½”ë“œ
            import matplotlib.pyplot as plt
            
            def visualize_test_accuracy_by_group(db):
                # âœ… 3ê·¸ë£¹ ë¶„ë¥˜
                groups = {
                    "Fine tuning": [
                        "googlenet_test_acc",
                        "googlenet_finetuned_test_acc",
                        "googlenet_gradual_unfreeze_test_acc"
                    ],
                    "LabelSmoothing Only": [
                        "googlenet_smooth_test",
                        "googlenet_finetuned_smooth_test",
                        "googlenet_gu_smooth_test"
                    ],
                    "LabelSmoothing + ReduceLROnPlateau": [
                        "googlenet_smooth_rlrop",
                        "googlenet_finetuned_smooth_rlrop",
                        "googlenet_gu_smooth_rlrop"
                    ]
                }
            
                # âœ… 3ê°œì˜ ì„œë¸Œí”Œë¡¯ ìƒì„±
                fig, axs = plt.subplots(1, 3, figsize=(18, 6), sharey=True)
            
                for i, (group_name, model_names) in enumerate(groups.items()):
                    accs = []
                    labels = []
            
                    for name in model_names:
                        acc = None
                        if name in db.list_collection_names():
                            result = db[name].find_one({"type": "final_test"})
                            if result and "test_accuracy" in result:
                                acc = result["test_accuracy"]
                        accs.append(acc if acc is not None else 0.0)
                        labels.append(name.replace("_test", ""))
            
                    # ğŸ“Š ë§‰ëŒ€ê·¸ë˜í”„
                    bars = axs[i].bar(labels, accs, color='mediumseagreen')
                    axs[i].set_title(group_name, fontsize=13)
                    axs[i].set_ylim(0, 100)
                    axs[i].tick_params(axis='x', rotation=20)
                    axs[i].grid(axis='y', linestyle='--', alpha=0.3)
                    axs[i].set_ylabel("Test Accuracy (%)" if i == 0 else "")
            
                    for bar, acc in zip(bars, accs):
                        label = f"{acc:.2f}%" if acc > 0 else "N/A"
                        axs[i].text(bar.get_x() + bar.get_width() / 2, bar.get_height() + 1,
                                    label, ha='center', va='bottom', fontsize=9)
            
                plt.suptitle("GoogLeNet Model Accuracy Comparison by Strategy", fontsize=15)
                plt.tight_layout()
                plt.show()
            visualize_test_accuracy_by_group(db)
            ```
            
        - í…ŒìŠ¤íŠ¸ í™•ì¸ & ì‹œê°í™” ì§„í–‰
            
            ```python
            import matplotlib.pyplot as plt
            
            def visualize_all_test_accuracies_flexible(model_names, db):
                test_accs = []
                labels = []
            
                for name in model_names:
                    acc = None
            
                    # ìš°ì„  name+'_test' ì»¬ë ‰ì…˜ì—ì„œ ì°¾ì•„ë³´ê¸°
                    test_collection_name = name + "_test"
                    if test_collection_name in db.list_collection_names():
                        result = db[test_collection_name].find_one({"type": "final_test"})
                        if result and "test_accuracy" in result:
                            acc = result["test_accuracy"]
            
                    # ê·¸ê²Œ ì•ˆë˜ë©´ name ì»¬ë ‰ì…˜ì—ì„œ ì°¾ì•„ë³´ê¸°
                    if acc is None and name in db.list_collection_names():
                        result = db[name].find_one({"type": "final_test"})
                        if result and "test_accuracy" in result:
                            acc = result["test_accuracy"]
            
                    test_accs.append(acc if acc is not None else 0.0)
                    labels.append(name)
            
                # ì‹œê°í™”
                plt.figure(figsize=(12, 6))
                bars = plt.bar(labels, test_accs, color='cornflowerblue')
                for bar, acc in zip(bars, test_accs):
                    label = f"{acc:.2f}%" if acc > 0 else "N/A"
                    plt.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 1,
                             label, ha='center', va='bottom', fontsize=9)
            
                plt.ylim(0, 100)
                plt.ylabel("Test Accuracy (%)")
                plt.title("GoogLeNet Overall Model 9 Testing Accuracy Comparison")
                plt.xticks(rotation=10, ha='right')
                plt.grid(axis='y', linestyle='--', alpha=0.2)
                plt.tight_layout()
                plt.show()
                
                all_9_models = [
                "googlenet_test_acc",
                "googlenet_finetuned_test_acc",
                "googlenet_gradual_unfreeze_test_acc",
                "googlenet_smooth",
                "googlenet_finetuned_smooth",
                "googlenet_gu_smooth",
                "googlenet_smooth_rlrop",
                "googlenet_finetuned_smooth_rlrop",
                "googlenet_gu_smooth_rlrop"
            ]
            
            visualize_all_test_accuracies_flexible(all_9_models, db)
            ```
            
        
        ![image.png](attachment:69e3912d-8b8f-4027-95eb-b73be26618a4:image.png)
        
        ![á„‰á…³á„á…³á„…á…µá†«á„‰á…£á†º 2025-03-30 á„‹á…©á„’á…® 9.53.55.png](attachment:e88aa547-330d-4d2f-9448-0703dfa335db:á„‰á…³á„á…³á„…á…µá†«á„‰á…£á†º_2025-03-30_á„‹á…©á„’á…®_9.53.55.png)
        
    - ì´ 9ê°€ì§€ ëª¨ë¸ì— ëŒ€í•˜ì—¬ ì´ í…ŒìŠ¤íŠ¸ ì •í™•ë„ë¥¼ ì‹œê°í™” í•˜ì˜€ë‹¤(ì•„ë˜).
        
        ![image.png](attachment:984fd20f-35d6-42a3-8d8c-30b5abfaee70:image.png)
        
        ![ê·¸ë£¹ë³„ ì‹œê°í™” ì§„í–‰](attachment:890efe2e-aec9-4e0b-9009-653c0935ae70:image.png)
        
        ê·¸ë£¹ë³„ ì‹œê°í™” ì§„í–‰
        

## 5. ì‹¤í—˜ ê²°ê³¼

---

- ëª¨ë¸ë³„ í…ŒìŠ¤íŠ¸ ì •í™•ë„ ê²°ê³¼ëŠ” ì•„ë˜ì™€ ê°™ë‹¤
    
    
    | **ëª¨ë¸ëª…** | **Test Accuracy (%)** |
    | --- | --- |
    | googlenet | 94.36 |
    | googlenet_finetuned | 82.60 |
    | googlenet_gradual_unfreeze | 91.42 |
    | `googlenet_smooth` | **97.06** |
    | googlenet_finetuned_smooth | 87.25 |
    | googlenet_gu_smooth | 85.78 |
    | googlenet_smooth_rlrop | 87.99 |
    | `googlenet_finetuned_smooth_rlrop` | **96.81** |
    | googlenet_gu_smooth_rlrop | 88.24 |
- ì¸ì‚¬ì´íŠ¸
    - **`Label Smoothing`**ì€ ì¼ë°˜ì ì¸ GoogLeNet í•™ìŠµë³´ë‹¤ **ë¶„ëª…í•œ ì„±ëŠ¥ í–¥ìƒ**ì„ ë³´ì„ (Base ê¸°ì¤€ 94.36 â†’ 97.06%)
    - **`Gradual Unfreeze`**ëŠ” ì •ê·œí™” ê³„ì—´ ê¸°ë²•(Loss Smoothing, ReduceLROnPlateau ë“±)ê³¼ì˜ ì¡°í•© ì‹œ **ê³¼ë„í•œ ì¼ë°˜í™”ë¡œ ì„±ëŠ¥ í•˜ë½** ê°€ëŠ¥ì„±
    - **`ReduceLROnPlateau`**ëŠ” overfitting ë°©ì§€, val_loss ì•ˆì •í™”ì— ìœ ë¦¬í•¨ â†’ ì¼ë¶€ ëª¨ë¸ì—ì„œ í° í­ì˜ ì •í™•ë„ ìƒìŠ¹
- ê³ ë ¤ì‚¬í•­
    - `googlenet_finetuned`  ëª¨ë¸ì€ ì˜¤íˆë ¤ ì •í™•ë„ ì €í•˜ â†’ **ì´ˆê¸° í•™ìŠµë¥  / ê³¼ì í•©** ê°€ëŠ¥ì„± ì¶”ì •
    - Gradual Unfreeze ì „ëµì€ **label smoothing ì—†ì´ ì ìš©** ì‹œ ë” ìœ ë¦¬í•  ìˆ˜ë„ ìˆìŒ
    - ì •ê·œí™”/ìŠ¤ì¼€ì¤„ë§ ê¸°ë²•ì´ í•­ìƒ ì´ì ì´ ë˜ì§€ ì•ŠìŒì„ ë³´ì—¬ì£¼ëŠ” ì˜ˆì‹œ í¬í•¨

## 6. ê²°ë¡ 

---

- ë³¸ í”„ë¡œì íŠ¸ëŠ” ì—¬ëŸ¬ ëª¨ë¸ê¸°ë²•ì¤‘, ë‚®ì€ ì •í™•ë„ë¥¼ ë³´ì¸ GoogLeNetì˜ ê¸°ë³¸ êµ¬ì¡°ì—ì„œ ì¶œë°œí•˜ì—¬,ë³´ì¡° ë¶„ë¥˜ê¸° í™œì„±í™”, Gradual Unfreeze, Label Smoothing, í•™ìŠµë¥  ìŠ¤ì¼€ì¤„ë§(ReduceLROnPlateau) ê¸°ë²•ì„ ì ì§„ì ìœ¼ë¡œ ì ìš©í•˜ë©´ì„œ ëª¨ë¸ ì„±ëŠ¥ì„ ë‹¨ê³„ì ìœ¼ë¡œ í–¥ìƒì‹œí‚¤ëŠ” ê²ƒì„ ëª©í‘œë¡œ í•˜ì˜€ìŒ
    
    íŠ¹íˆ **Finetuning**+**Label Smoothing + ReduceLROnPlateau** ì¡°í•©ì€ ì‘ì€ ë°ì´í„°ì…‹ í™˜ê²½ì—ì„œ íš¨ê³¼ì ì´ì—ˆìœ¼ë©°, GoogLeNet 1ì°¨ íŒŒì¸íŠœë‹ ëª¨ë¸ì—ì„œ ë†’ì€ í…ŒìŠ¤íŠ¸ ì •í™•ë„ì¸ 96.81%ë¥¼ ë‹¬ì„±í•˜ì˜€ìŒ
    
    ì´ëŠ” ì „ì²´ì ì¸ íŒŒë¼ë¯¸í„° í•™ìŠµì„ í†µí•´ ì‚¬ì „í•™ìŠµëœ ë°±ë³¸(Backbone) + Fully Connected + ë³´ì¡° ë¶„ë¥˜ê¸°(aux) ê°€ ëª¨ë‘ ì—…ë°ì´íŠ¸ ë˜ë©´ì„œ ëª¨ë¸ì˜ í‘œí˜„ë ¥ê³¼ ì ì‘ë ¥ì´ ìµœëŒ€ë¡œ ë°œíœ˜ëœ ê²ƒìœ¼ë¡œ ë³´ì„
    
    ì¶”ê°€ë¡œ `ReduceLROnPlateau`ê°€ ì¼ì • Epoch ì´í›„ val_loss ê°œì„ ì´ ë©ˆì¶”ë©´ í•™ìŠµë¥ ì„ ì ˆë°˜ìœ¼ë¡œ ë‚®ì¶”ì–´ ë” ì •ë°€í•œ í•™ìŠµìœ¼ë¡œ ì „í™˜ëœ ì ì„ ê³ ë ¤í•´ ê³¼ì í•© íƒ€ì´ë°ì„ ì¡ì•„ì¤€ ê²ƒìœ¼ë¡œ ë³´ì„
    
    í•˜ì§€ë§Œ ì¼ë°˜ GoogLeNetëª¨ë¸ì—ì„œëŠ” ì˜¤íˆë ¤ ì •í™•ë„ê°€ ë–¨ì–´ì§€ëŠ” ê²ƒì„ í™•ì¸í•  ìˆ˜ ìˆëŠ”ë°(97.06%â†’87.99%), ì´ëŠ” ë°ì´í„°ì˜ í´ë˜ìŠ¤ ìˆ˜ê°€ 3ê°œë¡œ ì ê¸°ì— ì˜¤íˆë ¤ Â ****`Label Smoothing`ì˜ ì •ê·œí™” íš¨ê³¼ ê³¼ë‹¤ë¡œ ë³´ì„
    
    ë˜í•œ ì¼ë°˜ GoogLeNet ëª¨ë¸ì€ ë³´ì¡°ë¶„ë¥˜ê¸°(aux)ê°€ ë¹„í™œì„±í™” ëœ ìƒíƒœì¸ë°, **Label Smoothingì´ ê³¼ë„í•˜ê²Œ confidenceë¥¼ ë‚®ì¶”ê³ ** ReduceLROnPlateauê°€ ì´ë¥¼ ë°”íƒ•ìœ¼ë¡œ ì¡°ê¸°ì— learning rateë¥¼ ë–¨ì–´ëœ¨ë¦¬ë©´ì„œ ê²°ê³¼ì ìœ¼ë¡œ **ì¶©ë¶„í•œ í•™ìŠµì„ ë°©í•´í•œ ê²ƒìœ¼ë¡œ ë³´ì„**
    
    ë”°ë¼ì„œ ìµœì  ì „ëµì€ ê¸°ë³¸ `GoogLeNet` ëª¨ë¸ì— **Label Smoothing ì ìš©** (googlenet_smooth) í•˜ê±°ë‚˜ **1ì°¨ íŒŒì¸íŠœë‹ + Label Smoothing + ReduceLROnPlateau ì¡°í•©**(googlenet_finetuned_smooth_rlrop)ìœ¼ë¡œ í•˜ëŠ” ê²ƒì´ ì ì ˆí•˜ë‹¤ê³  íŒë‹¨ëœë‹¤.
    

---

ì½”ë“œ ë§í¬:

- ResNet50 ëª¨ë¸ í•™ìŠµ ì½”ë“œ

[Google Colab](https://colab.research.google.com/drive/1KaoYjCGbDyiXGhz4ub2yvm5UlFvT-TE9?usp=sharing)

- ì´ì™¸ ì—¬ëŸ¬ ëª¨ë¸ í•™ìŠµ & ë¹„êµ ì½”ë“œ

[Google Colab](https://colab.research.google.com/drive/1TswtCKCh0EAiYc3kv_udFya8xf6MnGlN?usp=sharing)

- ìµœì¢… ëª¨ë¸ë§ & í•™ìŠµ ì½”ë“œ

[Google Colab](https://colab.research.google.com/drive/1-8qLkn-sNGnwFZhwF_BoKuInyJPNzWt8?usp=sharing)
